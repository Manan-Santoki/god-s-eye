# ═══════════════════════════════════════════════════════════════════════════════
# GOD_EYE OSINT Platform — Environment Configuration Template
# ───────────────────────────────────────────────────────────────────────────────
# SETUP:  cp .env.example .env  then fill in your values
# SECURITY: NEVER commit .env to version control (it is in .gitignore)
#           All keys marked (required) must be set for that module.
#           Modules without keys are auto-skipped — the app never crashes.
#
# PRIORITY ORDER (highest wins):
#   1. Process environment variables
#   2. .env file  ← this file
#   3. Default values in app/core/config.py
# ═══════════════════════════════════════════════════════════════════════════════


# ── Application ───────────────────────────────────────────────────────────────
APP_ENV=development                     # development | production
LOG_LEVEL=INFO                          # DEBUG | INFO | WARNING | ERROR | CRITICAL
LOG_FORMAT=text                         # json (structured) | text (human-readable)
API_PORT=8000                           # REST API listen port
DATA_DIR=./data                         # Base directory for all output data
DATA_RETENTION_DAYS=90                  # Days before scan data is auto-purged
MAX_CONCURRENT_MODULES=10              # Max modules running in parallel per scan
MAX_CONCURRENT_BROWSERS=3              # Max Playwright browser contexts open at once
REQUEST_TIMEOUT_SECONDS=30             # Default HTTP request timeout (seconds)
RESPECT_ROBOTS_TXT=true                # Honour robots.txt (set false for research use)
AUDIT_LOG_ENABLED=true                 # Write audit trail for all scans
CONSENT_REQUIRED=true                  # Show ethical-use consent banner on first run


# ── Infrastructure ────────────────────────────────────────────────────────────
# Neo4j — entity relationship graph (optional but recommended)
# Local: docker run -p7474:7474 -p7687:7687 -e NEO4J_AUTH=neo4j/god_eye_password neo4j
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=god_eye_password

# Redis — job queue, pub/sub, rate-limit counters
# Local: docker run -p6379:6379 redis:7-alpine
REDIS_URL=redis://localhost:6379


# ── VPN / Gluetun ─────────────────────────────────────────────────────────────
# Gluetun routes ALL browser and API traffic through a VPN tunnel.
# Docs: https://github.com/qdm12/gluetun/wiki
# Set VPN_ENABLED=true and configure docker-compose.yml gluetun service.
VPN_ENABLED=false
VPN_PROVIDER=nordvpn                   # nordvpn | expressvpn | protonvpn | mullvad | surfshark | privateinternetaccess | windscribe | ipvanish
VPN_TYPE=wireguard                     # wireguard | openvpn

# WireGuard credentials (when VPN_TYPE=wireguard)
WIREGUARD_PRIVATE_KEY=                 # WireGuard private key from your provider
WIREGUARD_ADDRESSES=                   # Assigned VPN address  e.g. 10.5.0.2/32

# OpenVPN credentials (when VPN_TYPE=openvpn)
VPN_USERNAME=
VPN_PASSWORD=

# Server selection
VPN_COUNTRIES=United States            # Target country for VPN exit node
VPN_CITIES=                            # Optional: specific city  e.g. New York
GLUETUN_INCOMING_PORTS=               # Optional: port forwarding (leave blank unless needed)

# Internal proxy URL exposed by Gluetun container — change only if port-mapping differs
GLUETUN_HTTP_PROXY=http://gluetun:8888


# ── Email Intelligence APIs ───────────────────────────────────────────────────
# Have I Been Pwned — https://haveibeenpwned.com/API/Key
# ~$3.50/month for full API access (well worth it for breach detection)
HIBP_API_KEY=

# Hunter.io — https://hunter.io  (free: 25 verifications/month)
# Domain email discovery + individual email verification
HUNTER_IO_API_KEY=

# DeHashed — https://dehashed.com  ($0.01/query, excellent breach DB)
DEHASHED_EMAIL=                        # Your DeHashed login email
DEHASHED_API_KEY=

# EmailRep — https://emailrep.io  (free tier: limited req/day)
# Email reputation, spam history, social presence
EMAILREP_API_KEY=

# Intelligence X — https://intelx.io  (free tier: 10 searches/day)
# Dark web + data leak search engine
INTELX_API_KEY=


# ── Search Engine APIs ────────────────────────────────────────────────────────
# SerpApi — https://serpapi.com/  (100 free searches/month, $50/month for 5k)
# PRIMARY search engine: Google results + image search + dork queries + LinkedIn/Instagram discovery
SERPAPI_API_KEY=
# Accepted aliases (use any one — they are interchangeable):
# SERPAPI_KEY=
# SERP_API_KEY=

# Optional: restrict SerpApi results to a specific geographic region.
# Examples: "United States", "India", "United Kingdom", "Mumbai,India"
# Leave blank for global results (recommended unless scanning a regional target).
SERPAPI_LOCATION=

# Override SerpApi base URL (leave blank for default)
SERPAPI_BASE_URL=https://serpapi.com/search.json

# Bing Web Search — https://azure.microsoft.com/en-us/services/cognitive-services/bing-web-search-api/
# Free tier: 1000 transactions/month
BING_API_KEY=

# Shodan — https://account.shodan.io/  ($59 one-time membership or free API)
# Internet-wide port/banner scanning; used for IP intelligence
SHODAN_API_KEY=

# VirusTotal — https://www.virustotal.com/gui/my-apikey
# Free: 4 req/min, 500/day — domain/IP/URL reputation
VIRUSTOTAL_API_KEY=

# Crawl4AI — optional AI-powered web content extraction service
# Self-host: docker run -p 11235:11235 unclecode/crawl4ai:latest
# Used by web modules for deep content extraction when standard scraping fails
CRAWL4AI_BASE_URL=                     # e.g. http://localhost:11235  (leave blank to disable)
CRAWL4AI_BEARER_TOKEN=                 # Optional auth token if your Crawl4AI instance requires it
CRAWL4AI_TIMEOUT_SECONDS=90           # Max seconds to wait for a Crawl4AI extraction job


# ── Social Media APIs ─────────────────────────────────────────────────────────
# GitHub Personal Access Token — https://github.com/settings/tokens
# Scopes needed: read:user, read:org (no write access required)
# Free, no expiry if you use a fine-grained token
GITHUB_TOKEN=

# Twitter/X Bearer Token — https://developer.twitter.com/en/apps
# Free tier: 1500 tweets/month read, tweet lookup
TWITTER_BEARER_TOKEN=

# Reddit App Credentials — https://www.reddit.com/prefs/apps  (create "script" app)
# Free: 60 requests/minute
REDDIT_CLIENT_ID=
REDDIT_CLIENT_SECRET=
REDDIT_USER_AGENT=GOD_EYE/1.0 by YourUsername   # Must be descriptive per Reddit rules

# YouTube Data API v3 — https://console.cloud.google.com/  → APIs & Services → YouTube Data API v3
# Free: 10,000 units/day (each search costs ~100 units)
YOUTUBE_API_KEY=


# ── Domain & Network APIs ─────────────────────────────────────────────────────
# WhoisXML API — https://www.whoisxmlapi.com/
# Free: 500 credits/month; used for WHOIS lookups and reverse WHOIS
WHOISXML_API_KEY=

# SecurityTrails — https://securitytrails.com/
# Free: 50 API calls/month; DNS history, subdomain enumeration
SECURITYTRAILS_API_KEY=

# IPinfo — https://ipinfo.io/account/token
# Free: 50,000 requests/month; IP geolocation, ASN, abuse contact
IPINFO_TOKEN=

# AbuseIPDB — https://www.abuseipdb.com/account/api
# Free: 1000 requests/day; IP abuse confidence scoring
ABUSEIPDB_API_KEY=

# Censys — https://censys.io/register
# Free: 250 queries/month; TLS certificate + host discovery
CENSYS_API_ID=
CENSYS_API_SECRET=


# ── Phone Intelligence ────────────────────────────────────────────────────────
# Numverify — https://numverify.com/  (free: 100 requests/month)
# International phone number validation + carrier lookup
NUMVERIFY_API_KEY=

# Twilio Lookup — https://www.twilio.com/  ($0.005 per lookup)
# Carrier, line type, caller ID
TWILIO_ACCOUNT_SID=
TWILIO_AUTH_TOKEN=


# ── Image Intelligence ────────────────────────────────────────────────────────
# TinEye Reverse Image Search — https://services.tineye.com/
# Paid: ~$200/year for 5,000 searches; finds where images appear online
TINEYE_API_KEY=

# Google Cloud Vision API — https://console.cloud.google.com/  → Vision API
# Free: 1,000 units/month; face detection, landmark recognition, OCR
GOOGLE_VISION_API_KEY=


# ── Business Intelligence ─────────────────────────────────────────────────────
# OpenCorporates — https://opencorporates.com/api_accounts/new
# Free: 500 requests/month; company registry data from 140+ jurisdictions
OPENCORPORATES_API_TOKEN=

# Clearbit — https://clearbit.com/
# Company/person enrichment from email or domain; requires paid plan
CLEARBIT_API_KEY=


# ── Browser Automation Credentials ───────────────────────────────────────────
# IMPORTANT: Use dedicated RESEARCH accounts — NEVER your personal accounts.
#            These accounts will browse, scrape, and may be rate-limited or
#            flagged by the platform's bot detection systems.
#
# Session state is persisted to data/sessions/<platform>_state.json so you
# only need to login once. Delete the state file to force a fresh login.
#
# LinkedIn — used by linkedin_scraper module (Phase 4)
# Create a throwaway LinkedIn account for research purposes.
LINKEDIN_EMAIL=
LINKEDIN_PASSWORD=

# Instagram — used by instagram_scraper module (Phase 4)
# Create a throwaway Instagram account for research purposes.
INSTAGRAM_USERNAME=
INSTAGRAM_PASSWORD=

# Facebook — used by facebook_scraper module (Phase 4)
# Create a throwaway Facebook account for research purposes.
FACEBOOK_EMAIL=
FACEBOOK_PASSWORD=

# TikTok — session cookie approach (safer than username/password for TikTok)
# 1. Log into TikTok in your browser
# 2. Open DevTools → Application → Cookies → tiktok.com
# 3. Copy the value of the 'sessionid' cookie
TIKTOK_SESSION_ID=


# ── Proxy Configuration ───────────────────────────────────────────────────────
# GOD_EYE proxy priority: Gluetun VPN (if VPN_ENABLED=true) > proxy list > TOR > none
USE_PROXY=false
PROXY_LIST_FILE=proxies.txt            # Path to proxy list file; one proxy per line
                                       # Format: protocol://user:pass@host:port
                                       # Example: socks5://alice:s3cr3t@proxy.example.com:1080
PROXY_ROTATION_STRATEGY=round_robin   # round_robin | random | least_used

# TOR Integration — requires a local TOR daemon
# Install: apt-get install tor  then: systemctl start tor
TOR_ENABLED=false
TOR_SOCKS_PORT=9050                    # SOCKS5 port (default: 9050)
TOR_CONTROL_PORT=9051                  # Control port for circuit renewal
TOR_PASSWORD=                          # Set in /etc/tor/torrc with HashedControlPassword
                                       # Generate: tor --hash-password "yourpassword"


# ── AI Configuration ──────────────────────────────────────────────────────────
# GOD_EYE uses AI for three tasks:
#   1. Correlation — linking entities across modules (names, emails, usernames, IPs)
#   2. Report generation — narrative summary + risk assessment
#   3. Vision analysis — reading screenshots/profile images for intelligence
#
# RECOMMENDED: OpenRouter gives access to 200+ models with one API key,
#              including Claude 3.5, GPT-4o, Gemini, and open-source models.

AI_PROVIDER=openrouter                 # openrouter (recommended) | anthropic | openai | ollama

# OpenRouter — https://openrouter.ai/keys  (pay-per-token, no subscription)
OPENROUTER_API_KEY=                    # sk-or-v1-...
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_SITE_URL=                   # Optional: your site URL sent in X-Title header (e.g. https://yoursite.com)
OPENROUTER_APP_NAME=GOD_EYE           # Identifies your app in OpenRouter dashboard

# Model selection — use different models for different tasks to balance cost/quality
# Vision model: needs multimodal capability (image understanding)
OPENROUTER_VISION_MODEL=anthropic/claude-3-5-sonnet    # Best: claude-3-5-sonnet | gpt-4o | gemini-pro-vision
# Report model: needs strong reasoning and long-context support
OPENROUTER_REPORT_MODEL=anthropic/claude-3-5-sonnet   # Best: claude-3-5-sonnet | gpt-4o | deepseek/deepseek-r1

# General AI model (used when not overridden by vision/report specific models)
AI_MODEL=anthropic/claude-3-5-sonnet

# Direct provider keys (only used when AI_PROVIDER is set to their name)
ANTHROPIC_API_KEY=                     # https://console.anthropic.com/  (claude-3-5-sonnet etc.)
OPENAI_API_KEY=                        # https://platform.openai.com/api-keys  (gpt-4o etc.)

# Ollama — self-hosted, fully local, free, no data sent externally
# Install: curl -fsSL https://ollama.ai/install.sh | sh  then: ollama pull llama3
OLLAMA_ENDPOINT=http://localhost:11434
OLLAMA_MODEL=llama3                    # llama3 | mistral | mixtral | llama3:70b | codellama

# AI feature toggles
AI_MAX_TOKENS=4000                     # Max tokens per AI response (increase for long reports)
ENABLE_AI_CORRELATION=true            # Cross-module entity correlation and deduplication
ENABLE_AI_REPORTS=true                # AI-generated narrative reports with risk assessment
ENABLE_AI_VISION=true                 # AI analysis of profile screenshots and downloaded images
